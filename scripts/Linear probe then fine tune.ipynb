{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'unlabeled_extrapolation.models' from '/juice/scr/ananya/cifar_experiments/unlabeled_extrapolation/unlabeled_extrapolation/models/__init__.py'>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we can load files from other sub-directories, e.g. datasets.\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path = [module_path] + sys.path\n",
    "module_path = os.path.abspath(os.path.join('../unlabeled_extrapolation'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path = [module_path] + sys.path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "import quinine\n",
    "from unlabeled_extrapolation.baseline_train import preprocess_config, get_all_test_stats, get_test_loaders, build_model\n",
    "from unlabeled_extrapolation.log_reg_sk import inv_normalize_weights\n",
    "import unlabeled_extrapolation.utils.utils as utils\n",
    "import unlabeled_extrapolation.datasets.breeds\n",
    "from unlabeled_extrapolation.models import set_linear_layer\n",
    "\n",
    "import calibration as cal\n",
    "import unlabeled_extrapolation\n",
    "import unlabeled_extrapolation.models\n",
    "import pickle\n",
    "\n",
    "import importlib\n",
    "importlib.reload(imnet_resnet)\n",
    "importlib.reload(unlabeled_extrapolation.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sklearn weights.\n",
    "path = '/u/scr/ananya/cifar_experiments/unlabeled_extrapolation/logs/linprobe_living17_resnet50/weights_0.pkl'\n",
    "coef, intercept, best_c, best_i = pickle.load( open( path, \"rb\" ) )\n",
    "# TODO: important, this is not needed long term so comment this line and uncomment the next line.\n",
    "# Only used as a stop-grab because we didn't apply this before saving.\n",
    "# new_coef, new_intercept = inv_normalize_weights(coef, intercept, features, 0)\n",
    "new_coef, new_intercept = coef, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get moco model, and initialize datasets.\n",
    "config_path = '/u/scr/ananya/cifar_experiments/unlabeled_extrapolation/configs/adaptation/living17.yaml'\n",
    "config = quinine.Quinfig(config_path)\n",
    "preprocess_config(config, config_path)\n",
    "config.train_dataset.classname = 'unlabeled_extrapolation.' + config.train_dataset.classname\n",
    "for test_dataset_config in config.test_datasets:\n",
    "    test_dataset_config.classname = 'unlabeled_extrapolation.' + test_dataset_config.classname\n",
    "config.model.args.checkpoint_path = '/u/scr/ananya/simclr_weights/moco_v2_800ep_pretrain.pth.tar'\n",
    "net = build_model(config)\n",
    "net.cuda()\n",
    "test_loaders, max_examples = get_test_loaders(config, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2977,  0.3007,  0.0680,  0.0970, -0.2821,  0.0656,  0.0192, -0.1751,\n",
       "        -0.1088,  0.2491, -0.2712, -0.2794,  0.2055,  0.5799, -0.3717,  0.0535,\n",
       "        -0.4479], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert sklearn model into pytorch.\n",
    "unlabeled_extrapolation.models.set_linear_layer(net._model.fc, new_coef, new_intercept)\n",
    "# coef_tensor = torch.tensor(new_coef, dtype=net._model.fc.weight.dtype).cuda()\n",
    "# bias_tensor = torch.tensor(new_intercept, dtype=net._model.fc.bias.dtype).cuda()\n",
    "# coef_param = torch.nn.parameter.Parameter(coef_tensor)\n",
    "# bias_param = torch.nn.parameter.Parameter(bias_tensor)\n",
    "# net._model.fc.weight = coef_param\n",
    "# net._model.fc.bias = bias_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': -1,\n",
       " 'test_loss/source_val_living': 0.1388502937776071,\n",
       " 'test_acc/source_val_living': 0.9647058823529412,\n",
       " 'test_loss/target_val_living': 0.6023032312039975,\n",
       " 'test_acc/target_val_living': 0.8194117647058824}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get test accuracies.\n",
    "device = 'cuda'\n",
    "criterion = utils.initialize(config['criterion']) \n",
    "get_all_test_stats(\n",
    "    epoch=-1, test_loaders=test_loaders, max_test_examples=max_examples,\n",
    "    config=config, net=net, criterion=criterion, device=device,\n",
    "    loss_name_prefix='test_loss/', acc_name_prefix='test_acc/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check this, let's load features and test weights using sklearn.\n",
    "features_path = '/u/scr/ananya/cifar_experiments/unlabeled_extrapolation/logs/linprobe_living17_resnet50/features_0'\n",
    "features, labels, loader_names = pickle.load( open( features_path, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_coef, new_intercept = inv_normalize_weights(coef, intercept, features, 0)\n",
    "clf = LogisticRegression(random_state=0, warm_start=True)\n",
    "clf.coef_ = new_coef\n",
    "clf.intercept_ = new_intercept\n",
    "clf.classes_ = np.array(list(range(17)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9647058823529412\n",
      "0.8194117647058824\n"
     ]
    }
   ],
   "source": [
    "for idx in [1, 2]:\n",
    "    preds = clf.predict(features[idx])\n",
    "    print(np.mean(preds == labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
