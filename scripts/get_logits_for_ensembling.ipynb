{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'unlabeled_extrapolation.utils.utils' from '/juice/scr/ananya/cifar_experiments/unlabeled_extrapolation/unlabeled_extrapolation/utils/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we can load files from other sub-directories, e.g. datasets.\n",
    "import os\n",
    "import sys\n",
    "from importlib import reload  \n",
    "\n",
    "module_path = '/u/scr/ananya/cifar_experiments/unlabeled_extrapolation'\n",
    "USE_CUDA = True\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import torch\n",
    "import unlabeled_extrapolation.utils.utils as utils\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import calibration as cal\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available: True\n"
     ]
    }
   ],
   "source": [
    "print(f'cuda is available: {torch.cuda.is_available()}')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "living17_config = {\n",
    "    'name': 'living17_lp_ft_ensemble',\n",
    "    'models':\n",
    "    [{\n",
    "        'type': 'ft',\n",
    "        'path_prefix': module_path + '/logs/full_ft_living17_resnet50/' + 'optimizer.args.lr-0.001_seed-{}_run{}/',\n",
    "        'checkpoint_suffix': 'ckp_best_source_val_living',\n",
    "        'id_index': 0,\n",
    "        'ood_index': 1\n",
    "     },\n",
    "     {\n",
    "        'type': 'lp',\n",
    "        'path_prefix': module_path + '/logs/linprobe_living17_resnet50/',\n",
    "        'id_index': 1,\n",
    "        'ood_index': 2\n",
    "     },\n",
    "    ],\n",
    "    'num_trials': 3,\n",
    "    'id_val_size': 1000,\n",
    "}\n",
    "\n",
    "entity30_config = {\n",
    "    'name': 'entity30_lp_ft_ensemble',\n",
    "    'models':\n",
    "    [{\n",
    "        'type': 'ft',\n",
    "        'path_prefix': module_path + '/logs/full_ft_entity30_resnet50/' + 'optimizer.args.lr-0.001_seed-{}_run{}/',\n",
    "        'checkpoint_suffix': 'ckp_best_source_val_entity',\n",
    "        'id_index': 0,\n",
    "        'ood_index': 1\n",
    "     },\n",
    "     {\n",
    "        'type': 'lp',\n",
    "        'path_prefix': module_path + '/logs/linprobe_entity30_resnet50/',\n",
    "        'id_index': 1,\n",
    "        'ood_index': 2\n",
    "     },\n",
    "    ],\n",
    "    'num_trials': 3,\n",
    "    'id_val_size': 500,\n",
    "}\n",
    "\n",
    "domainnet_config = {\n",
    "    'name': 'domainnet_lp_ft_ensemble',\n",
    "    'models':\n",
    "    [{\n",
    "        'type': 'ft',\n",
    "        'path_prefix': module_path + '/logs/full_ft_domainnet_clip_resnet50/' + 'optimizer.args.lr-0.001_seed-{}_run{}/',\n",
    "        'checkpoint_suffix': 'ckp_best_sketch_val',\n",
    "        'id_index': 0,\n",
    "        'ood_index': 1\n",
    "        \n",
    "     },\n",
    "     {\n",
    "        'type': 'lp',\n",
    "        'path_prefix': module_path + '/logs/linprobe_domainnet_clip_resnet50/',\n",
    "        'id_index': 4,\n",
    "        'ood_index': 3\n",
    "     },\n",
    "    ],\n",
    "    'num_trials': 3,\n",
    "    'id_val_size': 500,\n",
    "    \n",
    "}\n",
    "\n",
    "cifar_stl_config = {\n",
    "    'name': 'cifar_stl_lp_ft_ensemble',\n",
    "    'models':\n",
    "    [{\n",
    "        'type': 'ft',\n",
    "        'path_prefix': module_path + '/logs/full_ft_cifar_stl_resnet50/' + 'optimizer.args.lr-0.001_seed-{}_run{}/',\n",
    "        'checkpoint_suffix': 'ckp_best_cifar10-test',\n",
    "        'id_index': 0,\n",
    "        'ood_index': 1\n",
    "     },\n",
    "     {\n",
    "        'type': 'lp',\n",
    "        'path_prefix': module_path + '/logs/linprobe_cifar_stl_resnet50/',\n",
    "        'id_index': 1,\n",
    "        'ood_index': 3\n",
    "     },\n",
    "    ],\n",
    "    'num_trials': 3,\n",
    "    'id_val_size': 500,\n",
    "}\n",
    "\n",
    "all_configs = [entity30_config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "def load_model(config, checkpoint_path):\n",
    "    if 'unlabeled_extrapolation' not in config['model']['classname']:\n",
    "        config['model']['classname'] = 'unlabeled_extrapolation.' + config['model']['classname']\n",
    "    net = utils.initialize(config['model'])\n",
    "    if USE_CUDA:\n",
    "        net = net.cuda()\n",
    "    net.new_last_layer(config['num_classes'])\n",
    "    utils.load_ckp(checkpoint_path, net)\n",
    "    return net\n",
    "\n",
    "# Load datasets.\n",
    "def load_test_dataset(config, idx, batch_size=32, num_workers=2):\n",
    "    test_config = config['test_datasets'][idx]\n",
    "    if 'unlabeled_extrapolation' not in test_config['classname'] and 'torchvision' not in test_config['classname']:\n",
    "        test_config['classname'] = 'unlabeled_extrapolation.' + test_config['classname']\n",
    "    if 'transforms' not in test_config:\n",
    "        test_config['transforms'] = config['default_test_transforms']\n",
    "    test_data = utils.init_dataset(test_config)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=batch_size,\n",
    "        shuffle=False, num_workers=num_workers)\n",
    "    return test_data, test_loader\n",
    "\n",
    "def get_outputs_labels(net, loader):\n",
    "    net.cuda()\n",
    "    net.eval()\n",
    "    outputs_list, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = net(images)\n",
    "            outputs_list.append(outputs.detach().cpu().numpy())\n",
    "            labels_list.append(labels.detach().cpu().numpy())\n",
    "    outputs = np.concatenate(outputs_list)\n",
    "    labels = np.concatenate(labels_list)\n",
    "    return outputs, labels\n",
    "\n",
    "def split_dataset(inputs, labels, split_len):\n",
    "    # Returns (inputs1, labels1), (inputs2, labels2)\n",
    "    assert(len(inputs) == len(labels))\n",
    "    rng = np.random.RandomState(0)\n",
    "    random_indices = rng.permutation(len(inputs))\n",
    "    val_indices = random_indices[:split_len]\n",
    "    test_indices = random_indices[split_len:]\n",
    "    return (inputs[val_indices], labels[val_indices]), (inputs[test_indices], labels[test_indices])\n",
    "\n",
    "def make_validation(logits_list, labels_list, id_val_size):\n",
    "    (val_logits, val_labels), (test_logits, test_labels) = split_dataset(\n",
    "        logits_list[0], labels_list[0], split_len=id_val_size)\n",
    "    return [val_logits, test_logits] + logits_list[1:], [val_labels, test_labels] + labels_list[1:]\n",
    "\n",
    "def lp_to_all_logits_labels(model_config, config):\n",
    "    path_prefix = model_config['path_prefix']\n",
    "    all_labels_list, all_model_logits = [], []\n",
    "    for i in range(config['num_trials']):\n",
    "        print(i)\n",
    "        torch.cuda.empty_cache()\n",
    "        features_path = path_prefix + 'features_' + str(i)\n",
    "        features, labels, loader_names = pickle.load(open(features_path, 'rb'))\n",
    "        weights_path = path_prefix + 'weights_' + str(i) + '.pkl'\n",
    "        coef, intercept, _, _ = pickle.load(open(weights_path, 'rb'))\n",
    "        features_list = [features[model_config['id_index']], features[model_config['ood_index']]]\n",
    "        labels_list = [labels[model_config['id_index']], labels[model_config['ood_index']]]\n",
    "        print(loader_names, model_config['id_index'], model_config['ood_index'])\n",
    "        model_logits = [f @ coef.T + intercept for f in features_list]\n",
    "        model_logits, labels_list = make_validation(\n",
    "            model_logits, labels_list, id_val_size=config['id_val_size'])\n",
    "        all_model_logits.append(model_logits)\n",
    "        all_labels_list.append(labels_list)\n",
    "    return all_model_logits, all_labels_list\n",
    "\n",
    "def ft_to_all_logits_labels(model_config, config):\n",
    "    path_prefix = model_config['path_prefix']\n",
    "    checkpoint_suffix = model_config['checkpoint_suffix']\n",
    "    all_labels_list, all_model_logits = [], []\n",
    "    for i in range(config['num_trials']):\n",
    "        print(i)\n",
    "        torch.cuda.empty_cache()\n",
    "        cur_prefix = path_prefix.format(i, i)\n",
    "        config_path = cur_prefix + 'config.json'\n",
    "        ckp_path = cur_prefix + 'checkpoints/' + checkpoint_suffix\n",
    "        cur_config = load_config(config_path)\n",
    "        model = load_model(cur_config, ckp_path)\n",
    "        _, id_loader = load_test_dataset(cur_config, model_config['id_index'])\n",
    "        _, ood_loader = load_test_dataset(cur_config, model_config['ood_index'])\n",
    "        id_logits, id_labels = get_outputs_labels(model, id_loader)\n",
    "        ood_logits, ood_labels = get_outputs_labels(model, ood_loader)\n",
    "        model_logits = [id_logits, ood_logits]\n",
    "        labels_list = [id_labels, ood_labels]\n",
    "        model_logits, labels_list = make_validation(\n",
    "            model_logits, labels_list, id_val_size=config['id_val_size'])\n",
    "        all_model_logits.append(model_logits)\n",
    "        all_labels_list.append(labels_list)\n",
    "    return all_model_logits, all_labels_list\n",
    "\n",
    "def model_to_all_logits_labels(model_config, config):\n",
    "    if model_config['type'] == 'lp':\n",
    "        return lp_to_all_logits_labels(model_config, config)\n",
    "    elif model_config['type'] == 'ft':\n",
    "        return ft_to_all_logits_labels(model_config, config)\n",
    "    else:\n",
    "        raise ValueError('model_config must be lp or ft')\n",
    "\n",
    "def config_to_all_logits_labels(config):\n",
    "    # For each model, get logits, labels\n",
    "    rob_all_model_logits, all_labels_list = model_to_all_logits_labels(config['models'][1], config)\n",
    "    std_all_model_logits, all_labels_list2 = model_to_all_logits_labels(config['models'][0], config)\n",
    "    return std_all_model_logits, rob_all_model_logits, all_labels_list\n",
    "\n",
    "    # Split up the first dataset into 1000 and rest, using seed 0.\n",
    "#     return list(zip(*[get_outputs_labels(model, l) for l in loaders]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['source_train_entity', 'source_val_entity', 'target_val_entity'] 1 2\n",
      "1\n",
      "['source_train_entity', 'source_val_entity', 'target_val_entity'] 1 2\n",
      "2\n",
      "['source_train_entity', 'source_val_entity', 'target_val_entity'] 1 2\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "save_dir = '../logs/ensemble_logits/'\n",
    "for config in all_configs:\n",
    "    std_all_model_logits, rob_all_model_logits, all_labels_list = config_to_all_logits_labels(config)\n",
    "    pickle.dump((std_all_model_logits, rob_all_model_logits, all_labels_list),\n",
    "                open(save_dir + config['name']+'.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1700"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels_list[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/logs/full_ft_living17_resnet50/optimizer.args.lr-0.001_seed-2_run2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For fine-tuning get:\n",
    "\n",
    "# For linear-probing get:\n",
    "# features_i.pkl, weights_i.pkl, sub in i\n",
    "\n",
    "s = '/logs/full_ft_living17_resnet50/' + 'optimizer.args.lr-0.001_seed-{}_run{}'\n",
    "s.format(0, 0) # 0-index these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/logs/full_ft_living17_resnet50/optimizer.args.lr-0.001_seed-2_run2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
